\documentclass[review]{elsarticle}

\usepackage{lineno,hyperref}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{url}
\usepackage{subfig}
\DeclareMathOperator*{\argmin}{argmin}
\newcommand*{\argminl}{\argmin\limits}
\newtheorem{prop}{Proposition}

\modulolinenumbers[5]

\journal{Statistics and Probability Letters}

%%%%%%%%%%%%%%%%%%%%%%%
%% Elsevier bibliography styles
%%%%%%%%%%%%%%%%%%%%%%%
%% To change the style, put a % in front of the second line of the current style and
%% remove the % from the second line of the style you would like to use.
%%%%%%%%%%%%%%%%%%%%%%%

%% Numbered
%\bibliographystyle{model1-num-names}

%% Numbered without titles
%\bibliographystyle{model1a-num-names}

%% Harvard
%\bibliographystyle{model2-names.bst}\biboptions{authoryear}

%% Vancouver numbered
%\usepackage{numcompress}\bibliographystyle{model3-num-names}

%% Vancouver name/year
%\usepackage{numcompress}\bibliographystyle{model4-names}\biboptions{authoryear}

%% APA style
%\bibliographystyle{model5-names}\biboptions{authoryear}

%% AMA style
%\usepackage{numcompress}\bibliographystyle{model6-num-names}

%% `Elsevier LaTeX' style
\bibliographystyle{elsarticle-num}
%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}

\begin{frontmatter}

\title{A Stopped Negative Binomial Distribution}
\tnotetext[mytitlenote]{This research was supported by grants R01CA131301, 
R01CA157749, R01CA148996, R01CA168733, and PC50CA196530 awarded by the 
National Cancer Institute along with support from the Yale Comprehensive Cancer 
Center and the Yale Center for Outcomes Research. We would also like 
to thank Rick Landin at Ignyta Inc. for his suggestions.}

%% Group authors per affiliation:
%\author{Elsevier\fnref{myfootnote}}
%\address{Radarweg 29, Amsterdam}
%\fntext[myfootnote]{Since 1880.}

%% or include affiliations in footnotes:
\author[mymainaddress]{Michelle DeVeaux}
\ead{michelle.deveaux@yale.edu}

\author[mymainaddress]{Michael J. Kane\corref{mycorrespondingauthor}}
\cortext[mycorrespondingauthor]{Corresponding author}
\ead{michael.kane@yale.edu}

\author[mymainaddress]{Daniel Zelterman}
\ead{daniel.zelterman@yale.edu}

\address[mymainaddress]{Department of Biostatistics\\ School of Epidemiology and Public Health\\ Yale University, New Haven, CT}

\begin{abstract}
We introduce a discrete distribution suggested by curtailed
sampling rules common in early-stage clinical trials. We derive the
distribution of the smallest number of independent and identically
distributed Bernoulli trials needed to observe either $s$ successes 
or $t$ failures. The closed-form expression for the distribution as well as 
its characteristics are derived and properties of the distribution are explored.
\end{abstract}

\begin{keyword}
discrete distribution\sep curtailed sampling
\end{keyword}

\end{frontmatter}

\linenumbers

\section{Introdution and Motivation}

Consider a prototypical Phase II single-arm clinical trial in which 12 patients
are enrolled and treated. If two or more respond then we reject the null
hypothesis that the treatment is not effective and the trial proceeds to the 
next stage. If fewer that two respond then the null is not rejected and the 
trial is terminated.

If all 12 patients are enrolled at once, as in the classic
design, then the sample size is 12. However, as in most clinical trials, the
patients are enrolled sequentially, often with one patient's outcome realized
before the next one enters the trial. In the present example, observing two
successful patients allows us reach one endpoint so the sample required
could be as small as two. Similarly 11
observed treatment failures also ends the stage. This sampling mechanism, in
which the experiment ends as soon as any of the endpoints is reached, is
call {\em curtailed sampling}. Under curtailed sampling the range of the
sample size is between two and 12.

Assume each of patient outcome can be modeled as an independent,
identically distributed Bernoulli($p$) random variable. The trial is realized
as a sequence of these random variables that stops when either a
specified number of success or failures is reached. In the
previous example suppose two successes were reached after enrolling 10
patients (one in the third step and one at the $10^{th}$). The sample
path is illustrated
in Fig.~\ref{fig:kane_viz}. The vertical axis denotes the number of
successful outcomes. The horizontal axis counts the number of patients that
have been enrolled. The horizontal and vertical boundaries represent
endpoints for the trial.

\begin{figure}[t!]
\includegraphics[width=\textwidth]{KanePlot.pdf}
\caption{
A hypothetical realization of a trial.
}
\label{fig:kane_viz}
\end{figure}

\begin{table}[t!]
\caption{Stopped Negative Binomial Distribution Characteristics}
\label{tab:snb}
\begin{center}
\begin{tabular}{|c|l|} \hline
Notation & SNB($p$, $s$, $t$) \\ \hline
Parameters & $p$ the success probability ($q = 1-p$) \\
           & $s$ the number of successes before stopping \\
           & $t$ the number of failures before stopping \\ \hline
Support & min$(s,t) \leq k \leq s+t-1$  \\ \hline
PMF & ${k-1 \choose s-1} p^s (1-p)^{k-s} + {k-1 \choose t-1} (1-p)^s p^{k-t}$\\ \hline
CDF & $2 - \mathcal{I}_{1-p}(k+1, s) - \mathcal{I}_{p}(k+1, t)$\\ 
    & where $\mathcal{I}$ is the regularized incomplete beta function.\\ \hline
Mean & $\frac{s}{p} \mathcal{I}_p(s,t) + \frac{q^{t-1} p^{s-1}}{B(s,t)} +
  \frac{t}{q} \mathcal{I}_q(t,s) + \frac{q^{s-1}p^{t-1}}{B(s,t)}$\\ 
  & where $B$ is the beta function \\ \hline
MGF & $\left(\frac{p e^x}{1 - qe^x}\right)^s 
  \mathcal{I}_{1-qe^x} (s, t) + \left(\frac{qe^x}{1-pe^x}\right)^t 
  \mathcal{I}_{1-pe^x}(t, s) $\\ \hline
\end{tabular}
\end{center}
\end{table}

In this work we derive the distribution of the number of enrollees needed
for either $s$ successes or $t$ failures. We refer to this distribution
as the Stopped Negative Binomial (SNB) and some of its characteristics are
summarized in Tab.~\ref{tab:snb}.
The rest of this paper derives these results
and explores properties of the distribution.
The next section introduces our notation and basic results
including the density of the distribution along with a description of
it's relation to other distributions. Section 2 derives the distribution
based on a defined Bernoulli process and gives some basic properties.
Section 3 provides a connection to the Binomial tail probability.
Section 4 derives the posterior distribution using a Beta prior.
Section 5 provides a brief discussion on the use of the distribution
in clinical trials along with future avenues for generalization.

\section{Probability Mass Function}
\label{notation.section}

Let $\,b_1, b_2, \ldots \,$ denote a sequence of independent, identically
distributed, Bernoulli random variables with $\mathbb{P}[b_i=1]=p$ and
$\mathbb{P}[b_i = 0] = 1-p$, for
probability parameter $0\leq p \leq 1$. In the clinical trial setting
$\,b_i = 1$ corresponds to a successful outcome.  Let $s$ and $t$ be
positive integers.  Define the SNB random
variable $Y$ as the smallest
integer value such that $\,\{b_1, \ldots , b_Y\}\,$ contains {\em either}
$\,s\,$ successes {\em or} $\,t\,$ failures. That is, the SNB distribution
of $Y$ is the smallest integer such that either
$\sum_i^Y b_i = s$ or $\sum_i^Y 1-b_i = t$.

The distribution of $\,Y\,$ has support on integer values in the range
\begin{equation*}               
     \min(s,t) \leq \; Y \;\leq s+t-1  \label{range.y.eq}
\end{equation*}
and it is distributed as
\begin{equation} \label{eqn:pmf}
\mathbb{E}\  I_{\{Y=k\}} = S(k, p, s) \ I_{\{s \leq k\}} + 
  S(k, 1-p, t) \ I_{\{ t \leq k \}}
\end{equation}
where $I_{\{f\}}$ is one if $f$ is true and zero otherwise and
\begin{equation} \label{eqn:N}
S(k, p, s) = {k-1 \choose s-1} p^s (1-p)^{k-s} 
\end{equation}
is the negative binomial probability

To prove (\ref{eqn:pmf}), consider the
process $\mathbf{X} = \left\{X(k) : k = 0,1,... \right\}$
with $X(0)=0$ and
\begin{equation*} \label{eqn:proc}
X_{k+1} = X_k + b_{k+1} \ I_{\{ k-t < X_k < s\}}.
\end{equation*}
At each step a patient's outcome is measured. If it is success, the process 
advances one diagonally in the
positive horizontal and vertical direction.
Otherwise, it advances in the positive horizontal direction only. The
process continues until either $X_k = s$ or $X_k = k-t$.

\begin{prop}
The distribution of the stopping time
$\argminl_k \left[X_k \geq s \cup X_k \leq k-t \right]$
is given at (\ref{eqn:pmf}).
\end{prop}
\begin{proof}
%The proof will proceed in two parts. First, a combinatorial justification 
%will be given for the probability mass value on each element of the support. 
%Second, it will be shown that the sum of the masses over the support sums to 
%one.

The probability a given realization of $\mathbf{X}$ reaches $s$ at
the $k$th outcome is the probability that, at time $k-1$ there are $s-1$
successful outcomes and $k-s$ unsuccessful outcomes multiplied by
the probability of a success at time $k$. This expression is given
in (\ref{eqn:N}). 
Similarly, probability a given realization reaches $k-t$
is the probability that, at outcome $k-1$ there are $k-t$ successful outcomes
and $t-1$ unsuccessful outcomes multiplied by the probability of an
unsuccessful outcome at time $k$.  

Next, define
\begin{equation} \label{stop_t}
S'(k, p, t) = {k-1 \choose k-t} p^{k-t} (1-p)^t
\end{equation}
and notice that $S(k, p, s) = S'(k, 1-p, s)$ by writing
${k-1 \choose k-s} = {k-1 \choose s-1}$.

To show that (\ref{eqn:N}) and (\ref{stop_t}) sum to one
over their support, let
\begin{align} \label{eqn:sum_proof}
R &= \sum_{k=s}^{s+t-1} S(k, p, s) + \sum_{k=t}^{s+t-1} S(k, 1-p, t) \\
  &= \sum_{k=s}^{s+t-1} {k-1 \choose s-1} p^s (1-p)^{k-s} + \sum_{k=t}^{s+t-1} {k-1 \choose k-t} p^{k-t} (1-p)^t
\end{align}
where we substitute $i=k-s$ in the first summation and $j=k-t$ in the second.

Then $R$ can be written as the cumulative distribution function of two
negative binomial distributions:
\begin{equation} \label{eqn:transformed_sum}
R = \sum_{i=0}^{t-1} {i+s-1 \choose i} p^s (1-p)^i \; + \;
\sum_{j=0}^{s-1} {j+t-1 \choose j} p^j (1-p)^t.
\end{equation}

Let $\mathcal{I}_p(s, t)$ be the {\em regularized incomplete beta function} 
\citep{Olver2010} and recall this function satisfies 
$\mathcal{I}_p(s, t) = 1-\mathcal{I}_{1-p}(t, s)$ \citep{Abramowitz1964}.
\begin{align*}
R = \sum_{i=0}^{t-1} &{i+s-1 \choose i} p^s (1-p)^i +
\sum_{j=0}^{s-1}  {j+t-1 \choose j} p^j  (1-p)^t \\
   &= 1-\mathcal{I}_p(s, t) + 1 - \mathcal{I}_{1-p}(t, s) \\
   &= 1. 
\end{align*}
This completes the proof that (\ref{eqn:pmf}) is a valid probability mass
function.
\end{proof}

\section{Shape and Basic Properties}

\begin{figure}[p!]
\begin{center}
\includegraphics[width=\textwidth]{shapes.pdf}
\end{center}
\caption{Different shapes of the SNB distribution with parameters ($s$, $t$, $p$), as given. Red indicates mass contributed by hitting $s$, teal indicates
mass contributed by hitting $t$. \label{shapes.fig}}
\end{figure}

The SNB is a generalization of the negative 
binomial distribution. If $t$ is large then the $Y-s$ has a 
negative binomial distribution with
\begin{equation*}                                    %   (1)   
\mathbb{P}[Y=s+j]        \label{nb1.eq}          
  = {{s+j-1}\choose{s-1}} p^s (1-p)^j
\end{equation*}
for $\,j=0, 1,\ldots\,$. A similar statement can be made when $s$ is large
and $t$ is small. As a result, with proper parameter choice, the SNB
can mimic other probability distributions in a manner similar to 
those described in \cite{Peizer1968} and \cite{Best1974}. Examples are
shown in in Fig.~\ref{shapes.fig}. 

For the special case of $\,s=t,$ the distribution of $\,Y\,$ is the
riff-shuffle, or minimum negative binomial distribution~\citep{Uppuluri1970}.
Similar derivations of the closely-related maximum negative binomial 
discrete distributions also appear in~\cite{Zhang2000}
and \cite{Zelterman2005}.
The maximum negative binomial is the smallest number of outcomes necessary to 
observe at least $s$ successes {\em and} $s$ failures. The SNB is the 
number of coin flips to observe {\em either} $s$ heads or $t$ tails.

\section{Connection Between the SNB and the Binomial Tail Probability}

\begin{prop} \label{binomial_tail}
Let $Y$ be distributed as SNB($p$, $s$, $t$) and let 
$B$ be distributed Binomial with size $n=s+t-1$ and success probability
$p$. Then
\begin{equation}
\mathbb{P}[B \geq s] = \mathbb{P} [Y \leq n\ |\ \text{\#success} = s].
\end{equation}
That is, the probability that the number of successes is at least $s$
in the Binomial model is the same that the trial stops with $s$ 
successes in the SNB model.
\end{prop}
\begin{proof}
The Binomial tail probability is
\begin{align*}
\mathbb{P}[B \geq s] &= \sum_{k=s}^{s+t-1} {n \choose k} p^k (1-p)^{n-k} \\
  &= 1 - \sum_{k=0}^{s-1} {n \choose k} p^k (1-p)^{n-k} \\
  & 1 - \mathcal{I}_{1-p}.
\end{align*}
The corresponding SNB probability is
\begin{align*}
\mathbb{P} [Y \leq n\ |\ \text{\#success}=s] &= \sum_{k=s}^{s+t-1} S(k, p, s)\\
 &= {k-1 \choose s-1} p^s (1-p)^{k-s}.
\end{align*}
Let $i=k-s$. Use ${i+s-1 \choose s-1} = {i+s-1 \choose i}$
so the last summation can be rewritten as
\begin{align}
\mathbb{P} [Y \leq n\ |\ \text{\#success} = s] &= \sum_{i=0}^{t-1} 
  {i+s-1 \choose i} p^s (1-p)^i\\
  &= 1 - \mathcal{I}_{1-p}(t, s)
\end{align}
completing the proof.
\end{proof}

Fig.~\ref{fig:snb_bin_compare} shows the case
where $s=2$, $t=12$, and $p=0.2$. The probability masses represented in
red are equal as are the masses in teal. The probability that $s$
successes are reached in the SNB process is the same as the binomial 
probability of at least two successes. Likewise, the probability that $t$ 
failures are reached in the SNB process is the same as the binomial
probability of zero or one successes.

\begin{figure}[t!]
\centering
\subfloat[The SNB distribution.]{\includegraphics[width=0.5\textwidth]{snb_density.pdf}}
\hfill
\subfloat[The Binomial distribution.]{\includegraphics[width=0.5\textwidth]{bin_density.pdf}}
\caption{
SNB(0.2, 2, 12) with mass contributed from 
$s$ successes (red) or $t$ failures (teal) along with Bin(0.2, 13) with
at least 2 successes (red) or fewer (teal).
}
\label{fig:snb_bin_compare}
\end{figure}


\section{The Moment Generating Function}

\begin{prop} Let $Y$ be distributed SNB with parameters $p$, $s$, and $t$.
Then the moment generating function (MGF) of $Y$ is
\begin{equation} \label{eqn:mgf}
\mathbb{E}~e^{xY} = \left(\frac{p e^x}{1 - qe^x}\right)^s 
  \mathcal{I}_{1-qe^x} (s, t) + \left(\frac{qe^x}{1-pe^x}\right)^t 
  \mathcal{I}_{1-pe^x}(t, s)
\end{equation}
for $q = 1-p$ when $x \leq \min \left\{\log(1/p), \log(1/q) \right\}$.
\end{prop}
\begin{proof}
The MGF of the SNB is:
\begin{equation*}
\mathbb{E}~e^{xY} = \sum_{k=s}^{s+t-1} {k-1 \choose k-s} p^s q^{k-s} e^{kx} 
  + \sum_{k=t}^{s+t-1} {k-1 \choose k-t} p^{k-t} q^t e^{kx}
\end{equation*}
and can be rewritten as:
\begin{equation} \label{eqn:first_sum}
\mathbb{E}~e^{xY} = \sum_{k=s}^{s+t-1}{k-1 \choose k-s} (pe)^{sx} (qe^x)^{k-s} 
  + \sum_{k=t}^{s+t-1}{k-1 \choose k-t} (qe^x)^t (pe^x)^{k-t}.
\end{equation}
Taking the first summation in Equation \ref{eqn:first_sum}:
\begin{align*}
\sum_{k=s}^{s+t-1}{k-1 \choose k-s} (pe)^{sx} (qe^x)^{k-s} &= 
  \left(\frac{pe^x}{1 - qe^x}\right)^s \ \ \sum_{k=s}^{s+t-1} {k-1 \choose k-s} 
    (qe^x)^{k-s} (1-qe^x)^s \\
  &= \left(\frac{pe^x}{1 - qe^x}\right)^s \mathcal{I}_{qe^x}(s, t).
\end{align*}
Since the incomplete beta function has support on zero to one, we have
$qe^x \leq 1$. This also shows $x \leq -\log(q)$.

A similar expression can be derived using the same calculation with
the constraint that $x \leq -\log(p)$. The result
follows from the afore mentioned property of the regularized incomplete
beta function.
\end{proof}

\section{The Posterior Distribution}

Let us consider a Bayesian analysis where there is a Beta prior distribution
on $p$.
\begin{prop}
The posterior PMF of the Stopped Negative Binomial distribution with a Beta($\alpha$, $\beta$) prior is:
\begin{align} \label{eqn:posterior}
f(k | s, t, \alpha, \beta) &= 
  {k-1 \choose s-1} \frac{B\left(\alpha+s, k-s+\beta \right)}{B(\alpha, \beta)} 
    \ I_{\{s \leq k \leq s+k-1\}} \ + \nonumber \\
  & {k-1 \choose k-t} 
    \frac{B\left(\alpha + k - t, t+\beta\right)}{B(\alpha, \beta)} 
    \ I_{\{t \leq k \leq s+k-1\}}.
\end{align}
\end{prop}
\begin{proof}
For notational simplicity, assume that $s,t \leq k \leq s+t-1$. When this is not the case appropriate terms should be removed as dictated by the indicator functions.
\begin{align*}
f(k | s, t, \alpha, \beta) = \frac{1}{B(\alpha, \beta)} & \int_0^1 {k-1 \choose s-1} p^{\alpha +s -1} \left(1-p\right)^{k-s+\beta-1} + \\
 & {k-1 \choose k-t} p^{k-t+\alpha-1}\left(1-p\right)^{t+b-1} dp \\
= \frac{1}{B(\alpha, \beta)}  {k-1 \choose s-1} & \int_0^1  p^{\alpha +s -1} \left(1-p\right)^{k-s+\beta-1} dp + \\
 & \frac{1}{B(\alpha, \beta)} {k-1 \choose k-t} \int_0^1  p^{k-t+\alpha-1}\left(1-p\right)^{t+b-1} dp
\end{align*}
The result follows by applying the definition of the Beta function to the integral terms.
\end{proof}

\section{Discussion and Conclusion}

We have presented a new discrete distribution by curtailed sampling rules 
common in early-stage clinical trials, which we refer to as the Stopped 
Negative Binomial distribution. The distribution models the stopping time of 
a sequential trial where the trial is stopped when a number of events are 
accumulated. A connection between the Binomial tail probability and the SNB
distribution was shown; It's MGF was derived; and
the posterior distribution was derived for the case when the 
event probability $p$ has a Beta distribution. 

Current work focuses on two different areas. First, the distribution
is being applied to clinical trial design
where the number of enrollees is smaller than traditional trials.
This scenario is increasingly common in targeted cancer
therapy where treatment may only be appropriate for small subpopulations.
Second, generalizations and extensions of the distribution are being pursued
for applications power analysis. The posterior construction allows us
to quantify the uncertainty both in the response probability as well as the
time until a trial is complete. This suggests new approaches to sample
size calculations as well as trial monitoring.

\section*{References}

\bibliography{mybibfile}

\end{document}
